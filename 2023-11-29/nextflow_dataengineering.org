#+title: Nextflow Dataengineering

* Loved the modern biotech stack
Play on the modern datastack
* I was born in the year the term big data was invented
* thought the dagster tutorial was cool
But it's just plain python. Which is fine, but I take for granet that Nextflow isn't groovy it's a "DSL" written specifically for data pipelining


Imagining we'll pull from s3 from the nf-core megatests, do some stuff, push to motherduck

* TODO Querying data lake with trino to kick off a workflow
:PROPERTIES:
:CREATED:  [2023-10-20 Fri 12:16]
:END:
* Notes
- Load in CSV of rnaseq runs
- Run rnaseq
- Run multiqc
- Load up some multiqc parquet files and make a "data lake"

* Columnar Data storage
https://www.youtube.com/watch?v=2i2nyodhGkk&list=PLIYcNkSjh-0ztvwoAp3GeW8HNSUSk_q3K&index=8

[[file:img/why-columnar.png]]

TLDR only get the columns you want so faster
compression speeds things up even more!
* Duckdb is smart enough to not download the whole parquet file
https://www.youtube.com/watch?v=33sxkrt6eYk&list=PLIYcNkSjh-0ztvwoAp3GeW8HNSUSk_q3K&index=4

What about Fusion? Instead of httpfs
* Show Nextflow chunking a huge file and then making calls on it.
* Notes from showing Moni

** Topics
*** Antipatterns
*** Intro to parquet
*** Intro to DuckDB
- Half the presentation is why parquet is going to be cool
  - Might touch on, oh yeah if you slap iceberg on it in s3 it's even faster

*** MultiQC data files

- Not enough people use it
- Convert multiQC results to parquet
*** Note: Could also use Seqera platform and Jupyter/Rstudio
But this is Nextflow Summit, not Jupytercon
*** Snowflake is the _wrong_ choice
- Bioinformatics loves files
  - We're already outputting them, and we've built a datalake
    - We just gotta start using it.
