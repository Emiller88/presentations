#+title: Nextflow Dataengineering

* Loved the modern biotech stack
Play on the modern datastack
* I was born in the year the term big data was invented
* thought the dagster tutorial was cool
But it's just plain python. Which is fine, but I take for granet that Nextflow isn't groovy it's a "DSL" written specifically for data pipelining


Imagining we'll pull from s3 from the nf-core megatests, do some stuff, push to motherduck

* TODO Querying data lake with trino to kick off a workflow
:PROPERTIES:
:CREATED:  [2023-10-20 Fri 12:16]
:END:
* Notes
- Load in CSV of rnaseq runs
- Run rnaseq
- Run multiqc
- Load up some multiqc parquet files and make a "data lake"

* Columnar Data storage
https://www.youtube.com/watch?v=2i2nyodhGkk&list=PLIYcNkSjh-0ztvwoAp3GeW8HNSUSk_q3K&index=8

[[file:img/why-columnar.png]]

TLDR only get the columns you want so faster
compression speeds things up even more!
* Duckdb is smart enough to not download the whole parquet file
https://www.youtube.com/watch?v=33sxkrt6eYk&list=PLIYcNkSjh-0ztvwoAp3GeW8HNSUSk_q3K&index=4

What about Fusion? Instead of httpfs
* Show Nextflow chunking a huge file and then making calls on it.
* Notes from showing Moni

** Topics
*** Intro to DuckDB
- Half the presentation is why parquet is going to be cool
  - Might touch on, oh yeah if you slap iceberg on it in s3 it's even faster

*** Intro to parquet
#+begin_quote
Mark Schreiber
  7 days ago
parquet will give you some benefits from usually having better and faster compression than CSV due to column based compression. Where it really sings is the ability to only read specific columns or pages required to get only the data it needs for your analysis, potentially in parallel across nodes. Getting this benefit might depend a lot on how you implement a plugin. As others have said, it might be easier to query a DB or service that already knows how to get the most out of parquet and let it do all the heavy lifting.
#+end_quote
*** 1000 ducks
*** Antipatterns
**** Pressure of Nextflow headjob
*** MultiQC data files

- Not enough people use it
- Convert multiQC results to parquet
*** Note: Could also use Seqera platform and Jupyter/Rstudio
But this is Nextflow Summit, not Jupytercon
*** Snowflake is the _wrong_ choice
- Bioinformatics loves files
  - We're already outputting them, and we've built a datalake
    - We just gotta start using it.


* https://duckdb.org/2023/08/23/even-friendlier-sql.html
* Everyone will tell you the problem with duckdb is that it only runs on one computer
:PROPERTIES:
:CREATED:  [2023-11-17 Fri 15:47]
:END:

Y'all see where I'm going with this right?
* Simple query of Multiqc
s3://nf-core-awsmegatests.s3-eu-west-1.amazonaws.com/rnaseq/results-964425e3fd8bfc3dc7bce43279a98d17a874d3f7/aligner_star_rsem/multiqc/star_rsem/multiqc_data/multiqc_samtools_stats.txt
#+begin_src sql
CREATE TABLE reads AS SELECT Sample,reads_mapped_and_paired FROM read_csv_auto("s3://nf-core-awsmegatests/rnaseq/**/**/multiqc/star_rsem/multiqc_data/multiqc_samtools_stats.txt");
#+end_src
#+begin_src sql
CREATE TABLE reads
AS SELECT
    Sample,reads_mapped_and_paired
FROM read_csv_auto(
    "s3://nf-core-awsmegatests/sarek/*/test_full/multiqc/multiqc_data/multiqc_samtools_stats.txt",
    filename=true
);
#+end_src

https://duckdb.org/docs/extensions/httpfs#s3
* Notes from Talking with Abhinav
** When I say data engineering what does it make you think of?

ETL
** Bioinfo = data engineering
Compbio = Data science
** Could Query public dataservices
** [#A] Aggregate across multiple MultiQC runs
** Why not Athena(Trino)/BigQuery?
- We just don't need the overhead
*** TODO Follow up
Why not just R?
** We need to query
- During pre-launch
- In a providince way (nf-prov)


** Downside of using DuckDB is there's no data catalog
** nf-core use cases

- nf-co2 querying
- Finding a Module in multiple pipelines?

*** TODO Query https://github.com/nf-core/website/blob/main/.cache.tar.xz

For most used module

** TODO Samtools stats

- Pull the files in a make aquery
- Can update the names in a ~DDB~ fuction
- How many reads have been processed in nf-core mega tests?
** [#A] _It's a database with the headache_

**One sentence take-away**

You don't have to run one. They're all just files.
** TODO Move slides to google slides for Abhinav to review

** Outline
- Using DuckDB as a parquet reader
- Using it in processes as a Pandas replacement
- Starting to think in aggregates
- Generating samplesheets on the fly

** Aside: Talked about ad-hoc distributed clusters
** Talk with Moni after
*** DONE Medalion data structure
CLOSED: [2023-11-22 Wed 13:42]
- Organizing your datalake
- Broze is covered by all of the bioinformatics software
- MultiQC already covers silver
- Now we just need Gold
*** Perk of Galaxy over just Trino is the data discovery

Motherduck will be building this
*** We never needed MegaQC
- We can have this today. Do we really need something fancier?
