#+title: Outline

* Intro
* Intro to DuckDB
** Power of a DatabaseS without the headache
** L1 cache optimized
** R/Python/SQL/C
- You name it, there's a plugin
** PySpark compatible API
* Bioinformatics as data engineering
** Bioinformaticians love files

# My understanding from my young perspective is that all of the data wouldn't fit in memory at the time

** Medalion architecture
- Bronze = samtools_stats.txt
- Silver = multiqc_samtools_stats.txt
- Gold = ???

** We never needed MegaQC
- What if we could get 90% of the value from 10% of the work
- We can have this today. Do we really need something fancier?
- Maybe we could just add DuckDB-WASM to MultiQC and you can query in the browser?
* Nextflow Examples
** DuckDB as a Parquet Reader
** DuckDB in a process
*** Templating baked in
- Seeing all of the dbt stuff and thinking, that's kinda like a Nextflow process
*** Everyone will tell you the problem with duckdb is that it only runs on one computer
:PROPERTIES:
:CREATED:  [2023-11-17 Fri 15:47]
:END:

Y'all see where I'm going with this right?
1000 ducks
** DuckDB for Writing a samplesheet
** DuckDB and Nextflow anti-patterns

- Pressure on the head node
- Circle back around in a minute


* nf-core use cases

** nf-co2 querying
** Finding a Module in multiple pipelines?

- What's the most used module in all the pipelines

*** TODO Query https://github.com/nf-core/website/blob/main/.cache.tar.xz

For most used module

* DuckDB for querying the MultiQC Data Lake
** Simple query of Multiqc
#+begin_src sql
CREATE TABLE reads AS SELECT Sample,reads_mapped_and_paired FROM read_csv_auto("s3://nf-core-awsmegatests/rnaseq/*/multiqc/star_rsem/multiqc_data/multiqc_samtools_stats.txt");
#+end_src
** Starting to think in Aggregates

#+begin_src sql
CREATE TABLE reads
AS SELECT
    Sample,reads_mapped_and_paired
FROM read_csv_auto(
    "s3://nf-core-awsmegatests/sarek/*/test_full/multiqc/multiqc_data/multiqc_samtools_stats.txt",
    filename=true
);
#+end_src

- Pull the files in a make aquery
- Can update the names in a ~DDB~ fuction
- How many reads have been processed in nf-core mega tests?

https://duckdb.org/docs/extensions/httpfs#s3
** Benchmarking
Use Queries from DuckDB meet-up
*** HTTPFS vs Nextflow staging benchmark
*** HTTPFS with Parquet vs Nextflow staging benchmark
**** Quick Parquet Aside
***** Columnar Data storage
https://www.youtube.com/watch?v=2i2nyodhGkk&list=PLIYcNkSjh-0ztvwoAp3GeW8HNSUSk_q3K&index=8

[[file:img/why-columnar.png]]

TLDR only get the columns you want so faster
compression speeds things up even more!
***** Duckdb is smart enough to not download the whole parquet file
https://www.youtube.com/watch?v=33sxkrt6eYk&list=PLIYcNkSjh-0ztvwoAp3GeW8HNSUSk_q3K&index=4

What about Fusion? Instead of httpfs
*** HTTPFS with Iceberg vs Nextflow staging benchmark Iceberg
*** HTTPFS vs Fusion
*** Nextflow chunking a huge file and then making smaller DuckDB calls on it.
