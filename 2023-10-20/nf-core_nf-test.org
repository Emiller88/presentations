:PROPERTIES:
:DIR:      ./img/
:END:
#+title: nf-test at nf-core: empowering scalable and streamlined testing
#+author: Edmund Miller, Sateesh Peri, Maxime Garcia, Adam Talbot, Harshil Patel, JÃºlia Mir Pedrol, Nicolas Vannieuwkerke, and the nf-core maintainers
#+setupfile: ./setup.org

* TODO Add nf-core logo inside of the nf-test logo
* TODO Add in gregors custom version checker
Make sure mahesh didn't also work on it
* Abstract :noexport:

The nf-core community values results. One of the most important steps in
producing accurate analyses is incorporating reproducible and scalable testing
within your workflows. First weâ€™ll reflect on the beginning of testing at
nf-core, then dive into the evolution of testing starting with the need for a
testing framework, examine the introduction of modules, and understand the
adoption of pytest-workflow. Next, weâ€™ll go over how nf-test has been amazing
for testing full pipelines, their subworkflows and local modules. Weâ€™ll examine
some highlights such as snapshots, tags and the CI configuration. Finally, weâ€™ll
talk about nf-core modules, and the necessity to support legacy pytest-workflow
tests as we transition to nf-test. Weâ€™ll also go over the progress made during
the hackathon on nf-test.

note on [[id:54549341-c326-434f-be59-cdac2b846eeb][nf-test at nf-core]]


** About me

- Phd Candidate @ University of Texas at Dallas

*** My nf-core origin story :B_note:
:PROPERTIES:
:BEAMER_env: note
:END:

** Why Am I giving this talk :noexport:

# TODO Insert Spider man meme of Adam and Sateesh saying their the nf-test guru

# I think I got to give this talk because everyone else involved had something
# more important to present. These talks are tough to prepare for ahead of time
# because you're waiting for the hackathon to finish so you know what you can
# present
** Why does everyone keep talking about testing?

#+beamer: \pause
- Testing data pipelines is hard
#+beamer: \pause
- We really just want the bear-minimum, confidence in our results
#+beamer: \pause
- Bioinformatics software is finicky
** Figure 1: Nextflow enables stable analyses on different platforms.

[[https://media.springernature.com/lw685/springer-static/image/art%3A10.1038%2Fnbt.3820/MediaObjects/41587_2017_Article_BFnbt3820_Fig1_HTML.jpg?as=webp][Nextflow figure]]

* History of testing at nf-core

** Summary :noexport:

The nf-core community values results. One of the most important steps in
producing accurate analyses is incorporating reproducible and scalable testing
within your workflows. First we'll reflect on the beginning of testing at
nf-core, then dive into the evolution of testing starting with the need for a
testing framework, examine the introduction of modules, and understand the
adoption of pytest-workflow.



** TODO Evolution of nf-core Testing
# Imagining a like evolution of man but testing graphic

# Roll back to Methylseq
- In a land before CI
#+beamer: \pause
- Using GitHub actions to run ~nextflow run main.nf -profile test~
#+beamer: \pause
- Using pytest-workflow to test nf-core/modules
#+beamer: \pause
  - And check the md5sum of the outputs (and their existence!)
#+beamer: \pause
- A few clades in our phyogenetic tree
  - sarek using pytest-workflow to run ~nextflow run main.nf -profile test~
#+beamer: \pause
- ~nf-test~

** pytest-workflow for modules

\small
#+begin_src nextflow
workflow test_fastqc_paired_end {
  input = [
    [id: 'test', single_end: false], // meta map
    [
      file(params.test_data['sarscov2']['illumina']['test_1_fastq_gz']),
      file(params.test_data['sarscov2']['illumina']['test_2_fastq_gz'])
      ]
  ]
  FASTQC ( input )
}
#+end_src

** pytest-workflow for modules

\small
#+begin_src yaml
- name: fastqc test_fastqc_paired_end
  command: nextflow run ./tests/modules/nf-core/fastqc -entry test_fastqc_paired_end -c ./tests/config/nextflow.config -c ./tests/modules/nf-core/fastqc/nextflow.config
  tags:
    - fastqc
  files:
    - path: output/fastqc/test_1_fastqc.html
      contains:
        - <tr><td>File type</td><td>Conventional base calls</td></tr>
    - path: output/fastqc/test_1_fastqc.zip
    - path: output/fastqc/test_1.txt
      md5sum: 68db6114ef348f721c139cd4c9534f73
#+end_src

*** Notes :B_note:
:PROPERTIES:
:BEAMER_env: note
:END:

- As you can see, a lot of complexity and HUGE barrier to entry
- Time consuming and collecting files, and hashes by hand or writing scripts


** Somethings we learned along the way

- Checking what code has changed and only running tests for that code
  - /Speed/ - Saves developer time
  - GitHub runners are a finite resource
- Conda hates checksums
- Investing time in testing *pays off in the long run*

* nf-test Features we love
** nf-test Example

\small
#+begin_src nextflow
nextflow_pipeline {

  name "Test Hello World"
  script "nextflow-io/hello"

  test("hello world example should start 4 processes") {
    // ...
  }
}
#+end_src

** nf-test Example

#+begin_src Nextflow
  test("hello world example should start 4 processes") {
    expect {
      with(workflow){
        assert success
        // analyze Nextflow trace file
        assert trace.tasks().size() == 4
        // Verify if strings have been written to stdout object
        assert "Ciao world!" in stdout
        assert "Bonjour world!" in stdout
        assert "Hello world!" in stdout
        assert "Hola world!" in stdout
      }
    }
  }
#+end_src

** nf-test Example

#+begin_src bash
nf-test test pipeline.nf.test
#+end_src
** Simple Nextflow style one-line install

#+begin_src bash
curl -fsSL https://code.askimed.com/install/nf-test | bash
#+end_src

** Testing all Nextflow components and Test generation

#+begin_src bash
# Create a test cases for all processes in folder modules:
nf-test generate process modules/**/*.nf
# Create a test case for a sub workflow:
nf-test generate workflow workflows/some_workflow.nf
# Create a test case for the whole pipeline:
nf-test generate pipeline main.nf
# Create a test case for each functio in file functions.nf:
nf-test generate function functions.nf
#+end_src

** Snapshot generation

#+begin_src nextflow
assert snapshot(
    workflow,
    path("${params.outdir}/file1.txt"),
    path("${params.outdir}/file2.txt"),
    path("${params.outdir}/file3.txt")
).match()
#+end_src

#+beamer: \pause
#+begin_src bash
nf-test test tests/main.nf.test --update-snapshot
#+end_src

*** :B_note:
:PROPERTIES:
:BEAMER_env: note
:END:

This was manual or scripted to pull in the hashes by Kevin

** Powerful assertions - GZip files

#+begin_src bash
assert path(process.out.out_ch.get(0)).linesGzip.size() == 5
assert path(process.out.out_ch.get(0)).linesGzip.contains("Line Content")
#+end_src

** Powerful assertions - Fasta files (and Plugins!)

#+begin_src bash
assert path('path/to/fasta1.fasta').fasta == path("path/to/fasta2.fasta'").fasta
# Work with individual samples
def sequences = path('path/to/fasta1.fasta.gz').fasta
assert "seq1" in sequences
assert !("seq8" in sequences)
assert sequences.seq1 == "AGTACGTAGTAGCTGC"
#+end_src

* nf-test in modules

** nf-test in modules

- Only running CI on code that changed
- Splitting up jobs to run on separate runners

** GitHub runs in modules :ATTACH:


[[attachment:modules_workflows.png]]


** A smooth transition :ATTACH:

[[file:img/modules_activity.png]]

- We can't just stop updating modules


** The swap :ATTACH:

#+attr_latex: :height 0.7\linewidth
[[attachment:nf-core_swap.jpg]]

** Â¿Por QuÃ© No Los Dos? :ATTACH:

# TODO Swap these out for passing tests

[[attachment:modules_no-nf-test.png]]

# Â¿Por QuÃ© No Los Dos? meme?
# https://knowyourmeme.com/photos/166112

** In action :ATTACH:


[[attachment:modules_nftest_workflow.png]]

** Pre-hackathon

- 2 modules(fastqc and snakemake)
- Still supporting pytest-workflow
- Only running tests on changed code

** Future

- 53 modules and subworkflows tested
- Module maintainers with automated review requests
- Automated package updates

** IDEA Abstract :noexport:

Finally, we'll jump into nf-core modules, and how we couldn't just rip out
pytest-workflow. We'll briefly go over the ability to have both testing
frameworks available in the repo. Then we'll talk about the few issues holding
back the overall adoption of nf-test because of some bad habits we picked up
from pytest-workflow.

Hopefully then we'll get to take a quick victory lap for all of the modules we
converted over to nf-test during the hackathon
* nf-test in pipelines

** nf-core Introduction

- Nextflow pipelines with a twist
#+beamer: \pause
- Run on /everything/
#+beamer: \pause
- Supported by *template updates* when new version releases of nf-core/tools that
  usually add *new features*
#+beamer: \pause
  + These new features sometimes come with unintended bugs!
#+beamer: \pause
- Supported by pull-requests from the community
#+beamer: \pause
  - But how do we trust code from a new contributor?

** Why test pipelines?

- Avoid Regressions
- Improve confidence in scientific outcomes
- Confidence in collaboration across the world

** Where are we?

- Added in Methylseq in version ~2.4.0~
- Pre-hackathon: 7 pipelines
#+beamer: \pause
# ðŸŽ‰
- Post-hackathon: _11_ pipelines

*** Pipelines :noexport:
methylseq
fetchngs
ampliseq
demultiplex
nascent
mag
viralintegration
phageannotator
spatialtranscriptomics
createpanelrefs
bamtofastq
** GitHub Actions Runners are finite resources :ATTACH:

[[attachment:actions.png]]

** The problem with testing :ATTACH:


[[attachment:sarek_workflows.png]]

** nf-test scatter

# https://github.com/askimed/nf-test/issues/102

- Typically software tests are ran in parallel
- Usually each test doesn't take 5 minutes...

# TODO You get a test, and you get a test and you get test
# https://imgflip.com/memegenerator/Oprah-You-Get-A

** nf-test scatter :ATTACH:

#+attr_latex: :height 0.7\linewidth
[[attachment:workflows_scatter_example_tmp.png]]


** Where are we going?

- Lock down the nf-core nf-test standard(where do the tests live?)
- Add nf-test to the template
- Add test generation into ~nf-core <modules/subworkflows> create~

** IDEA Abstract :noexport:
Next, we'll go over how nf-test has been amazing for testing full pipelines,
their subworkflows and local modules. We'll examine some highlights such as
snapshots, tags and the CI configuration.

- We could never use pytest-workflow for testing pipelines(Expect Sarek)
- We'll mainly focus on the CI infrastructure, and configuration of these tests
  and how we parallelized the testing of the workflows.




* TODO Conclusion
