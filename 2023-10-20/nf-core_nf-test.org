:PROPERTIES:
:DIR:      ./img/
:END:
#+title: nf-test at nf-core: empowering scalable and streamlined testing
#+setupfile: ./setup.org

* Abstract :noexport:

The nf-core community values results. One of the most important steps in
producing accurate analyses is incorporating reproducible and scalable testing
within your workflows. First weâ€™ll reflect on the beginning of testing at
nf-core, then dive into the evolution of testing starting with the need for a
testing framework, examine the introduction of modules, and understand the
adoption of pytest-workflow. Next, weâ€™ll go over how nf-test has been amazing
for testing full pipelines, their subworkflows and local modules. Weâ€™ll examine
some highlights such as snapshots, tags and the CI configuration. Finally, weâ€™ll
talk about nf-core modules, and the necessity to support legacy pytest-workflow
tests as we transition to nf-test. Weâ€™ll also go over the progress made during
the hackathon on nf-test.

note on [[id:54549341-c326-434f-be59-cdac2b846eeb][nf-test at nf-core]]

* About Me :noexport:

# I think I got to give this talk because everyone else involved had something
# more important to present. These talks are tough to prepare for ahead of time
# because you're waiting for the hackathon to finish so you know what you can
# present
** Why Am I giving this talk

# TODO Insert Spider man meme of Adam and Sateesh saying their the nf-test guru

* History of testing at nf-core

** Summary :noexport:

The nf-core community values results. One of the most important steps in
producing accurate analyses is incorporating reproducible and scalable testing
within your workflows. First we'll reflect on the beginning of testing at
nf-core, then dive into the evolution of testing starting with the need for a
testing framework, examine the introduction of modules, and understand the
adoption of pytest-workflow.



** TODO Evolution of nf-core Testing
# Imagining a like evolution of man but testing graphic

# Roll back to Methylseq
- In a land before CI
#+beamer: \pause
- Using GitHub actions to run ~nextflow run main.nf -profile test~
#+beamer: \pause
- Using pytest-workflow to test nf-core/modules
#+beamer: \pause
  - And check the md5sum of the outputs (and their existence!)
#+beamer: \pause
- A few clades in our phyogenetic tree
  - sarek using pytest-workflow to run ~nextflow run main.nf -profile test~
#+beamer: \pause
- ~nf-test~

** Quick run through of pytest-workflow for modules

\small
#+begin_src nextflow
workflow test_fastqc_paired_end {
  input = [
    [id: 'test', single_end: false], // meta map
    [
      file(params.test_data['sarscov2']['illumina']['test_1_fastq_gz']),
      file(params.test_data['sarscov2']['illumina']['test_2_fastq_gz'])
      ]
  ]
  FASTQC ( input )
}
#+end_src

** Quick run through of pytest-workflow for modules

\small
#+begin_src yaml
- name: fastqc test_fastqc_paired_end
  command: nextflow run ./tests/modules/nf-core/fastqc -entry test_fastqc_paired_end -c ./tests/config/nextflow.config -c ./tests/modules/nf-core/fastqc/nextflow.config
  tags:
    - fastqc
  files:
    - path: output/fastqc/test_1_fastqc.html
      contains:
        - <tr><td>File type</td><td>Conventional base calls</td></tr>
    - path: output/fastqc/test_1_fastqc.zip
    - path: output/fastqc/test_1.txt
      md5sum: 68db6114ef348f721c139cd4c9534f73
#+end_src

*** Notes :B_note:
:PROPERTIES:
:BEAMER_env: note
:END:

- As you can see, a lot of complexity and HUGE barrier to entry
- Time consuming and collecting files, and hashes by hand or writing scripts

** We're not that sophisticated with our testing

- We really just want the bear-minimum, to make sure stuff runs!
- Make sure end-to-end results are consistent between versions.

** Somethings we learned along the way

- Checking what code has changed and only running tests for that code
  - /Speed/ - Saves developer time
  - GitHub runners are a finite resource
- Conda is a pain with hashes
- Investing time in testing leads to *improvements each release*

* nf-test Features we love
** nf-test Example

\small
#+begin_src nextflow
nextflow_pipeline {

  name "Test Hello World"
  script "nextflow-io/hello"

  test("hello world example should start 4 processes") {
    // ...
  }
}
#+end_src

** nf-test Example

#+begin_src Nextflow
  test("hello world example should start 4 processes") {
    expect {
      with(workflow){
        assert success
        // analyze Nextflow trace file
        assert trace.tasks().size() == 4
        // Verify if strings have been written to stdout object
        assert "Ciao world!" in stdout
        assert "Bonjour world!" in stdout
        assert "Hello world!" in stdout
        assert "Hola world!" in stdout
      }
    }
  }
#+end_src

** nf-test Example

#+begin_src bash
nf-test test pipeline.nf.test
#+end_src
** Simple Nextflow style one-line install

#+begin_src bash
curl -fsSL https://code.askimed.com/install/nf-test | bash
#+end_src

** Testing all Nextflow components and Test generation

#+begin_src bash
# Create a test cases for all processes in folder modules:
nf-test generate process modules/**/*.nf
# Create a test case for a sub workflow:
nf-test generate workflow workflows/some_workflow.nf
# Create a test case for the whole pipeline:
nf-test generate pipeline main.nf
# Create a test case for each functio in file functions.nf:
nf-test generate function functions.nf
#+end_src

** Snapshot generation

#+begin_src nextflow
assert snapshot(
    workflow,
    path("${params.outdir}/file1.txt"),
    path("${params.outdir}/file2.txt"),
    path("${params.outdir}/file3.txt")
).match()
#+end_src

#+beamer: \pause
#+begin_src bash
nf-test test tests/main.nf.test --update-snapshot
#+end_src


** Powerful assertions - GZip files

#+begin_src bash
assert path(process.out.out_ch.get(0)).linesGzip.size() == 5
assert path(process.out.out_ch.get(0)).linesGzip.contains("Line Content")
#+end_src

** Powerful assertions - Fasta files (and Plugins!)

#+begin_src bash
assert path('path/to/fasta1.fasta').fasta == path("path/to/fasta2.fasta'").fasta
# Work with individual samples
def sequences = path('path/to/fasta1.fasta.gz').fasta
assert "seq1" in sequences
assert !("seq8" in sequences)
assert sequences.seq1 == "AGTACGTAGTAGCTGC"
#+end_src

* nf-test in modules

** nf-test in modules

- Only running CI on code that changed
- Splitting up jobs to run on separate runners

** GitHub runs in modules :ATTACH:


[[attachment:modules_workflows.png]]


** A smooth transition :ATTACH:

[[file:img/modules_activity.png]]

- We can't just stop updating modules


** The swap :ATTACH:

#+attr_latex: :height 0.7\linewidth
[[attachment:nf-core_swap.jpg]]

** In action :ATTACH:

# TODO Swap these out for passing tests

[[attachment:modules_no-nf-test.png]]

# Â¿Por QuÃ© No Los Dos? meme?
# https://knowyourmeme.com/photos/166112

** In action :ATTACH:


[[attachment:modules_fastqc_workflow.png]]

** Pre-hackathon

- 2 modules(fastqc and snakemake)
- Still supporting pytest-workflow
- Only running tests on changed code

** Post-hackathon

- [[https://github.com/nf-core/modules/pull/3228][nf-core/modules#3228 Migrate standalone tests to nf-test by sateeshperi]]
- [[https://github.com/nf-core/modules/issues/3857][nf-core/modules#3857 Migrate subworkflow tests in nf-core/modules to nf-test]]
- [[https://github.com/nf-core/modules/issues/3858][nf-core/modules#3858 Migrate chained modules tests to nf-test]]
- Scatter-gather tests

** Outstanding issues

- Testing modules that are chained together
- Unzipping input files

** IDEA Abstract :noexport:

Finally, we'll jump into nf-core modules, and how we couldn't just rip out
pytest-workflow. We'll briefly go over the ability to have both testing
frameworks available in the repo. Then we'll talk about the few issues holding
back the overall adoption of nf-test because of some bad habits we picked up
from pytest-workflow.

Hopefully then we'll get to take a quick victory lap for all of the modules we
converted over to nf-test during the hackathon
* nf-test in pipelines

** Pipelines Introduction

- Nextflow pipelines with a twist
#+beamer: \pause
- Run on /everything/
#+beamer: \pause
- Supported by *template updates* when new version releases of nf-core/tools that
  usually add *new features*
#+beamer: \pause
  + These new features sometimes come with unintended bugs!
#+beamer: \pause
- Supported by pull-requests from the community
#+beamer: \pause
  - But how do we trust code from a new contributor?

** Why test pipelines?

- Avoid Regressions
- Improve confidence in scientific outcomes

** Where are we?

- Added in Methylseq in version ~2.4.0~
- Pre-hackathon: 7 pipelines
#+beamer: \pause
# ðŸŽ‰
- Post-hackathon: _?_ pipelines
  - (We have another 3 slated)

** GitHub Actions Runners are finite resources :ATTACH:

[[attachment:actions.png]]

** GitHub actions in sarek :ATTACH:


[[attachment:sarek_workflows.png]]

** nf-test scatter

# https://github.com/askimed/nf-test/issues/102

- Typically software tests are ran in parallel
- Usually each test doesn't take 5 minutes...

# TODO You get a test, and you get a test and you get test
# https://imgflip.com/memegenerator/Oprah-You-Get-A

** nf-test scatter :ATTACH:

#+attr_latex: :height 0.7\linewidth
[[attachment:workflows_scatter_example_tmp.png]]


** Post Hackathon

- Replace all basic CI tests with nf-test
  - [[https://github.com/nf-core/tools/pull/2007][nf-core/tools#2007 Add nf-test to pipeline template by Emiller88]]
- # of pipelines with basic CI tests
- # of pipelines with subworkflows tested
- # of pipelines with modules tested
- # of pipelines with functions tested

** Where are we going?

- Lock down the nf-core nf-test standard(where do the tests live?)
- Add nf-test to the template
- Add test generation into ~nf-core <modules/subworkflows> create~

** IDEA Abstract :noexport:
Next, we'll go over how nf-test has been amazing for testing full pipelines,
their subworkflows and local modules. We'll examine some highlights such as
snapshots, tags and the CI configuration.

- We could never use pytest-workflow for testing pipelines(Expect Sarek)
- We'll mainly focus on the CI infrastructure, and configuration of these tests
  and how we parallelized the testing of the workflows.




* TODO Conclusion
